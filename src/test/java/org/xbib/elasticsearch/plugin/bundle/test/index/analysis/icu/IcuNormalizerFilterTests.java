package org.xbib.elasticsearch.plugin.bundle.test.index.analysis.icu;

import com.ibm.icu.text.Normalizer2;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.MockTokenizer;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.core.KeywordTokenizer;
import org.elasticsearch.test.ESTokenStreamTestCase;
import org.xbib.elasticsearch.plugin.bundle.index.analysis.icu.IcuNormalizerFilter;

/**
 * ICU normalizer filter tests.
 */
public class IcuNormalizerFilterTests extends ESTokenStreamTestCase {

    public void testDefaults() throws Exception {
        Analyzer a = new Analyzer() {
            @Override
            public TokenStreamComponents createComponents(String fieldName) {
                Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
                return new TokenStreamComponents(tokenizer,
                        new IcuNormalizerFilter(tokenizer,
                                Normalizer2.getInstance(null, "nfkc_cf", Normalizer2.Mode.COMPOSE)));
            }
        };
        assertAnalyzesTo(a, "This is a test", new String[] { "this", "is", "a", "test" });
        assertAnalyzesTo(a, "Ru√ü", new String[] { "russ" });
        assertAnalyzesTo(a, "ŒúŒÜŒ™ŒüŒ£", new String[] { "ŒºŒ¨œäŒøœÉ" });
        assertAnalyzesTo(a, "ŒúŒ¨œäŒøœÇ", new String[] { "ŒºŒ¨œäŒøœÉ" });
        assertAnalyzesTo(a, "êêñ", new String[] { "êêæ" });
        assertAnalyzesTo(a, "Ô¥≥Ô¥∫Ô∞ß", new String[] { "ÿ∑ŸÖÿ∑ŸÖÿ∑ŸÖ" });
        assertAnalyzesTo(a, "‡§ï‡•ç‚Äç‡§∑", new String[] { "‡§ï‡•ç‡§∑" });
        a.close();
    }

    public void testAlternate() throws Exception {
        Analyzer a = new Analyzer() {
            @Override
            public TokenStreamComponents createComponents(String fieldName) {
                Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
                return new TokenStreamComponents(tokenizer, new IcuNormalizerFilter(
                        tokenizer,
                        Normalizer2.getInstance(null, "nfc", Normalizer2.Mode.DECOMPOSE)));
            }
        };
        assertAnalyzesTo(a, "\u00E9", new String[] { "\u0065\u0301" });
        a.close();
    }

    public void testEmptyTerm() throws Exception {
        Analyzer a = new Analyzer() {
            @Override
            protected TokenStreamComponents createComponents(String fieldName) {
                Tokenizer tokenizer = new KeywordTokenizer();
                return new TokenStreamComponents(tokenizer,
                        new IcuNormalizerFilter(tokenizer,
                                Normalizer2.getInstance(null, "nfkc_cf", Normalizer2.Mode.COMPOSE)));
            }
        };
        checkOneTerm(a, "", "");
        a.close();
    }
}
